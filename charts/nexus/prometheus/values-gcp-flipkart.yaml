# values.yaml for ck-prometheus-chart when using kube-prometheus-stack as a dependency

kube-prometheus-stack:
  nameOverride: "ck"
  fullnameOverride: "ck"

  global:
    imagePullSecrets:
      - name: ckn-ghcr-secret

  # CRDs are managed externally (manual / separate Argo app with SSA)
  crds:
    enabled: false

  prometheusOperator:
    enabled: true
    # Don't let the chart manage CRDs â€“ avoids Argo + annotation issues
    manageCRDs: false

    # Operator image from GHCR
    image:
      registry: ghcr.io
      repository: ckgitrepouser/codekarma/infra/prometheus-operator
      tag: v0.86.2
      pullPolicy: IfNotPresent

    # Config reloader image from GHCR
    prometheusConfigReloader:
      image:
        registry: ghcr.io
        repository: ckgitrepouser/codekarma/infra/prometheus-config-reloader
        tag: v0.86.2
        pullPolicy: IfNotPresent

    # Webhook / certgen images from GHCR
    admissionWebhooks:
      enabled: true
      cert:
        image:
          registry: ghcr.io
          repository: ckgitrepouser/codekarma/infra/kube-webhook-certgen
          tag: v1.6.4
          pullPolicy: IfNotPresent
      # If you later enable the patch job, keep it consistent:
      # patch:
      #   image:
      #     registry: ghcr.io
      #     repository: ckgitrepouser/codekarma/infra/kube-webhook-certgen
      #     tag: v1.6.4
      #     pullPolicy: IfNotPresent

  prometheus:
    enabled: true
    fullnameOverride: "ck"
    prometheusSpec:
      # Prometheus server image from GHCR
      image:
        registry: ghcr.io
        repository: ckgitrepouser/codekarma/infra/prometheus
        tag: v3.7.3
        pullPolicy: IfNotPresent

      maximumStartupDurationSeconds: 60
      podMonitorSelector: {}
      probeSelector: {}
      retention: 90d

      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: premium-rwo
            accessModes: [ "ReadWriteOnce" ]
            resources:
              requests:
                storage: 50Gi

      resources:
        requests:
          memory: "4096Mi"
          cpu: "4000m"
        limits:
          memory: "4096Mi"
          cpu: "4000m"

      # Node affinity for n2d nodes
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: cloud.google.com/machine-family
                    operator: In
                    values:
                      - n2d

      # GKE-specific tolerations
      tolerations:
        - key: "nodetype"
          operator: "Equal"
          value: "n2d"
          effect: "NoSchedule"

      securityContext:
        fsGroup: 2000
        runAsNonRoot: true
        runAsUser: 1000

      enableAdminAPI: true

    # Expose Prometheus via NodePort
    service:
      type: NodePort

  # Disable other components (but keep images wired to GHCR for future use)

  kubeApiServer:
    enabled: false

  prometheus-node-exporter:
    enabled: false
    image:
      registry: ghcr.io
      repository: ckgitrepouser/codekarma/infra/node-exporter
      tag: v1.10.2
      pullPolicy: IfNotPresent

  grafana:
    enabled: false
    image:
      registry: ghcr.io
      repository: ckgitrepouser/codekarma/infra/grafana
      tag: 12.3.0
      pullPolicy: IfNotPresent

  alertmanager:
    enabled: false
    alertmanagerSpec:
      image:
        registry: ghcr.io
        repository: ckgitrepouser/codekarma/infra/alertmanager
        tag: v0.29.0
        pullPolicy: IfNotPresent

  nodeExporter:
    enabled: false
    image:
      registry: ghcr.io
      repository: ckgitrepouser/codekarma/infra/node-exporter
      tag: v1.10.2
      pullPolicy: IfNotPresent

  kubeStateMetrics:
    enabled: false
    image:
      registry: ghcr.io
      repository: ckgitrepouser/codekarma/infra/kube-state-metrics
      tag: v2.17.0
      pullPolicy: IfNotPresent

  kubeControllerManager:
    enabled: false

  coreDns:
    enabled: false

  kubeEtcd:
    enabled: false

  kubeProxy:
    enabled: false

  kubeScheduler:
    enabled: false

  kubelet:
    serviceMonitor:
      enabled: false
    service:
      enabled: false

  defaultRules:
    create: false

  additionalServiceMonitors: []
